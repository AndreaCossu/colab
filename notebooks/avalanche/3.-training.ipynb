{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "Welcome to the \"_Training_\" tutorial of the \"_From Zero to Hero_\" series. In this part we will present the functionalities offered by the `training` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ContinualAI/avalanche.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 💪 The Training Module\n",
    "\n",
    "The `training` module in _Avalanche_ is build on modularity and its main goals are two:\n",
    "\n",
    "1. provide a set of standard **continual learning baselines** that can be easily run for comparison;\n",
    "2. provide the necessary utilities to **create and run your own strategy** as efficiently and easy as possible with building blocks we already prepared for you.\n",
    "\n",
    "At the moment the `training` module offers two submodule:\n",
    "\n",
    "* **Strategies**: it containes the collection of pre-implemented baselines you can use for comparisons and base classes to inherit to create your strategy.\n",
    "* **Plugins**: plugins are modules implementing some specific \\(and oftern reusable across strategies\\) behaviour you can attach to your own strategy.\n",
    "\n",
    "## 📈 Strategies\n",
    "\n",
    "If you want to compare your strategy with other classic continual learning algorithm or baselines, in _Avalanche_ this is as simply as instantiate an object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.strategies import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC\n",
    "\n",
    "model = SimpleMLP(num_classes=10)\n",
    "cl_strategy = Naive(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use Them\n",
    "\n",
    "Each strategy object offers two main methods `train` and `eval`. Both of them, accept either a _single experience_ \\(`Experience`\\) or a _list of them_, for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [4, 5]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 0 (Task 0) from train stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:53<00:00,  2.11it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8586\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 25.68it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1873\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9466\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 26.18it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 10.0613\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 25.77it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 9.8839\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 25.49it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 7.8305\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 25.61it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 9.5457\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 7.5909\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.1774\n",
      "Start of experience:  1\n",
      "Current Classes:  [8, 2]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 1 (Task 0) from train stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [00:07<00:00, 15.64it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2210\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6332\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 23.93it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.5216\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 24.03it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.3306\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9472\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 23.02it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 8.4007\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 23.09it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.9140\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 23.31it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 7.9442\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 6.0173\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.1900\n",
      "Start of experience:  2\n",
      "Current Classes:  [9, 6]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 2 (Task 0) from train stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [00:08<00:00, 14.42it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2352\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6457\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 23.76it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.3017\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 25.01it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 7.0971\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 25.73it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 0.2836\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.9583\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 24.60it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 7.3372\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 25.25it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 7.8322\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 5.9935\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.1885\n",
      "Start of experience:  3\n",
      "Current Classes:  [1, 7]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 3 (Task 0) from train stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:08<00:00, 15.92it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2212\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6666\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 26.28it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.9496\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 26.92it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 8.1148\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 30.08it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.6829\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 28.95it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 0.2839\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.9385\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 28.82it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 8.4721\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 6.1795\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.2030\n",
      "Start of experience:  4\n",
      "Current Classes:  [0, 3]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 4 (Task 0) from train stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 121/121 [00:07<00:00, 16.40it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3219\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6378\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 26.28it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.6633\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 26.85it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 8.7407\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 26.28it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 7.7548\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 25.11it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.0014\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 26.01it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.1912\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9613\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 6.0510\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.1913\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from avalanche.benchmarks.generators import nc_scenario\n",
    "from avalanche.models import SimpleMLP\n",
    "\n",
    "# --- CONFIG\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_experiences = 5\n",
    "# ---------\n",
    "\n",
    "# --- TRANSFORMATIONS\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "# ---------\n",
    "\n",
    "# --- SCENARIO CREATION\n",
    "mnist_train = MNIST('./data/mnist', train=True,\n",
    "                    download=True, transform=train_transform)\n",
    "mnist_test = MNIST('./data/mnist', train=False,\n",
    "                   download=True, transform=eval_transform)\n",
    "scenario = nc_scenario(\n",
    "    mnist_train, mnist_test, n_experiences, shuffle=True, seed=1234,\n",
    "    task_labels=False)\n",
    "# ---------\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
    "cl_strategy = Naive(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=100, train_epochs=1, eval_mb_size=100,\n",
    "    device=device)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    results.append(cl_strategy.eval(scenario.test_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝Create your Strategy\n",
    "\n",
    "In _Avalanche_ you can build your own strategy in 2 main ways:\n",
    "\n",
    "1. **From Scratch Mode**: This is the simplest way to build your own strategy, where you only have to create a python class that implements the main `train` and `eval` methods. However, in this case, all the plugins utilities \\(like the evaluation ones that we'll see in the follow up tutorial\\) cannot be automatically integrated in your algorithm. You should integrate them \"_manually_\".\n",
    "2. **Callbacks Mode**: At this level _Avalanche_ offers a `BaseStrategy` you can inherit from. This strategy offers a simple API and defines the _training and evaluation flows_ i.e. the order of functions to be called when the _train_ and _eval_ methods are triggered. Many of these functions have been already implemented for you in their basic form so you can define a new strategy simply by specializing a few of them. All the evaluation utilities are already integrated in the `BaseStrategy`.\n",
    "\n",
    "If the standard training and evaluation flows, for some reason, do not fit in your ideal continual learning strategy, you're free to change them as well, as simply as redefining the `train` and `eval` methods in the `BaseStrategy`.\n",
    "\n",
    "In general, we suggest to work with the _Callbacks Mode_, which offers in our opinion the right level of automation at a reasonable complexity, even for _Avalanche_ \"_newbies_\". But let's see an example for each of those modalities!\n",
    "\n",
    "### From Scratch Mode\n",
    "\n",
    "Let's define our Continual Learning algorithm \"_MyStrategy_\" as a simple python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyStrategy():\n",
    "    \"\"\"My Basic Strategy\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train(self, experience):\n",
    "        # here you can implement your own training loop for each experience (i.e.\n",
    "        # batch or task).\n",
    "\n",
    "        train_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        train_data_loader = DataLoader(\n",
    "            train_dataset, batch_size=128\n",
    "        )\n",
    "\n",
    "        for epoch in range(1):\n",
    "            for mb in train_data_loader:\n",
    "                # you magin here...\n",
    "                pass\n",
    "\n",
    "    def eval(self, experience):\n",
    "        # here you can implement your own evaluation loop for each experience\n",
    "        # (i.e. batch or task).\n",
    "\n",
    "        eval_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        eval_data_loader = DataLoader(\n",
    "            eval_dataset, batch_size=128\n",
    "        )\n",
    "\n",
    "        # eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use our strategy as we would do for the pre-implemented ones. However, as previously hinted, some evaluation utilities will be note included automatically and should be called by hand. Please refer to the follow-up notebook for more details about what the `evaluation` module can offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience  0\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "Start of experience  1\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "Start of experience  2\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "Start of experience  3\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "Start of experience  4\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.generators import nc_scenario\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- CONFIG\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_experiences = 5\n",
    "# ---------\n",
    "\n",
    "# --- TRANSFORMATIONS\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "# ---------\n",
    "\n",
    "# --- SCENARIO CREATION\n",
    "mnist_train = MNIST('./data/mnist', train=True,\n",
    "                    download=True, transform=train_transform)\n",
    "mnist_test = MNIST('./data/mnist', train=False,\n",
    "                   download=True, transform=eval_transform)\n",
    "scenario = nc_scenario(\n",
    "    mnist_train, mnist_test, n_experiences, shuffle=True, seed=1234,\n",
    "    task_labels=False)\n",
    "# ---------\n",
    "\n",
    "# MODEL CREATION\n",
    "model = SimpleMLP(num_classes=scenario.n_classes)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
    "cl_strategy = MyStrategy(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss())\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    cl_strategy.eval(scenario.test_stream[experience.current_experience])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Callbacks Mode\n",
    "\n",
    "At this level _Avalanche_ offers a `BaseStrategy` you can inherit from. This strategy offers a simple API and defines the _training and evaluation flows_ to help you build your own by specializing just a few methods.\n",
    "\n",
    "**But why do we need inheritance to implement a new continual learning algorithm?**\n",
    "\n",
    "We noticed that many continual learning strategy follows roughly the same training/evaluation flows and implement the same boilerplate code.\n",
    "\n",
    "So you it seems natural to define a new strategy just by specializing a few methods. If strategies are defined only \"_by difference_\" from a basic one, this will **reduce overhead and code duplication**, **improving code readability and prototyping speed**.\n",
    "\n",
    "**Base Strategy**\n",
    "\n",
    "As we previously mentioned _Training_ and _Evaluation flows_ are just sequences of functions called within the `BaseStrategy`.\n",
    "\n",
    "The **standard flows** defined in _Avalanche_ contain the following functions:\n",
    "\n",
    "```text\n",
    "train\n",
    "    before_training\n",
    "    before_training_exp\n",
    "    adapt_train_dataset\n",
    "    make_train_dataloader\n",
    "    before_training_epoch\n",
    "        before_training_iteration\n",
    "            before_forward\n",
    "            after_forward\n",
    "            before_backward\n",
    "            after_backward\n",
    "        after_training_iteration\n",
    "        before_update\n",
    "        after_update\n",
    "    after_training_epoch\n",
    "    after_training_exp\n",
    "    after_training\n",
    "```\n",
    "\n",
    "```text\n",
    "eval\n",
    "    before_eval\n",
    "    adapt_eval_dataset\n",
    "    make_eval_dataloader\n",
    "    before_eval_exp\n",
    "        eval_epoch\n",
    "            before_eval_iteration\n",
    "            before_eval_forward\n",
    "            after_eval_forward\n",
    "            after_eval_iteration\n",
    "    after_eval_exp\n",
    "    after_eval\n",
    "```\n",
    "\n",
    "The are all the functions contained in the flows as defined by the `BaseStrategy`, the basic class we suggest you to inherit for implementing your continual learning algorithms.\n",
    "\n",
    "For this class, all the methods \\(that you can of course override\\) in the flows are implemented for you. So, creating a plain `Replay` algorithm can be done as simply as defining the subclass below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.training.strategies import BaseStrategy\n",
    "\n",
    "\n",
    "class MyReplay(BaseStrategy):\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion,\n",
    "                 mem_size=200,\n",
    "                 evaluation_protocol=None,\n",
    "                 train_mb_size=1, train_epochs=1,\n",
    "                 eval_mb_size=None, device=None,\n",
    "                 plugins=None):\n",
    "\n",
    "        super().__init__(\n",
    "            model, optimizer, criterion, evaluation_protocol,\n",
    "            train_mb_size=train_mb_size, train_epochs=train_epochs,\n",
    "            eval_mb_size=eval_mb_size, device=device, plugins=plugins)\n",
    "\n",
    "        self.mem_size = mem_size\n",
    "        self.ext_mem = None\n",
    "        self.it = 0\n",
    "        self.rm_add = None\n",
    "\n",
    "    def adapt_train_dataset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Expands the current training set with datapoint from\n",
    "        the external memory before training.\n",
    "        \"\"\"\n",
    "\n",
    "        # remember to call the super method\n",
    "        super().adapt_train_dataset(**kwargs)\n",
    "\n",
    "        # Additional set of the current batch to be concatenated to the ext.\n",
    "        # memory at the end of the training\n",
    "        self.rm_add = None\n",
    "\n",
    "        # how many patterns to save for next iter\n",
    "        h = min(self.mem_size // (self.it + 1), len(self.current_data))\n",
    "\n",
    "        # We recover it using the random_split method and getting rid of the\n",
    "        # second split.\n",
    "        self.rm_add, _ = random_split(\n",
    "            self.current_data, [h, len(self.current_data) - h]\n",
    "        )\n",
    "\n",
    "        if self.it > 0:\n",
    "            # We update the train_dataset concatenating the external memory.\n",
    "            # We assume the user will shuffle the data when creating the data\n",
    "            # loader.\n",
    "            self.current_data = ConcatDataset([self.current_data,\n",
    "                                               self.ext_mem])\n",
    "\n",
    "    def after_training_exp(self, **kwargs):\n",
    "        \"\"\" After training we update the external memory with the patterns of\n",
    "         the current training batch/task. \"\"\"\n",
    "\n",
    "        # remember to call the super method\n",
    "        super().adapt_train_dataset(**kwargs)\n",
    "\n",
    "        # replace patterns in random memory\n",
    "        ext_mem = self.ext_mem\n",
    "        if self.it == 0:\n",
    "            ext_mem = copy.deepcopy(self.rm_add)\n",
    "        else:\n",
    "            _, saved_part = random_split(\n",
    "                ext_mem, [len(self.rm_add), len(ext_mem) - len(self.rm_add)]\n",
    "            )\n",
    "            ext_mem = ConcatDataset([saved_part, self.rm_add])\n",
    "        self.ext_mem = ext_mem\n",
    "        self.it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specializing a method from the parent class, **remember to call \"super\" one** for the necessarely strategy bookkeeping or to retain the original method behavior \\(check the `BaseStrategy` class for more details\\).\n",
    "\n",
    "**Join Training**\n",
    "\n",
    "When prototyping new continual learning algorithms we often want to understand how the same model would perform when trained on whole stream of data all together. This is often referred to as the \"_Cumulative_\", \"_Joint-training_\" or \"_Offline_\" upper bound.\n",
    "\n",
    "In Avalanche this can be done with another basic strategy you can extend called `JointTraining`. `JointTraining` follows roughtly the same ideas and API of the `BaseStrategy` but instead of processing experience in a streams iteratively it collapse them into a single, big dataset from which to learn.\n",
    "\n",
    "It supports both streams with one or multiple tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 0 (Task 0) from train stream --\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [04:23<00:00,  7.10it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2856\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8620\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:07<00:00, 39.54it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.2152\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9379\n",
      "-- Starting eval on experience 1 (Task 1) from test stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:07<00:00, 39.65it/s]\n",
      "> Eval on experience 1 (Task 1) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task001/Exp001 = 0.2091\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task001/Exp001 = 0.9396\n",
      "-- Starting eval on experience 2 (Task 2) from test stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:07<00:00, 39.69it/s]\n",
      "> Eval on experience 2 (Task 2) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task002/Exp002 = 0.2144\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task002/Exp002 = 0.9377\n",
      "-- Starting eval on experience 3 (Task 3) from test stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 38.74it/s]\n",
      "> Eval on experience 3 (Task 3) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task003/Exp003 = 0.2128\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task003/Exp003 = 0.9398\n",
      "-- Starting eval on experience 4 (Task 4) from test stream --\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:07<00:00, 39.46it/s]\n",
      "> Eval on experience 4 (Task 4) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task004/Exp004 = 0.2128\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task004/Exp004 = 0.9396\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 0.2129\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.9389\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from avalanche.benchmarks.classic import PermutedMNIST\n",
    "from avalanche.training.strategies import JointTraining\n",
    "\n",
    "# MNIST dataset urls\n",
    "prev_mnist_urls = torchvision.datasets.MNIST.resources\n",
    "new_resources = [\n",
    "    ('https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz', prev_mnist_urls[0][1]),\n",
    "    ('https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz', prev_mnist_urls[1][1]),\n",
    "    ('https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz', prev_mnist_urls[2][1]),\n",
    "    ('https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz', prev_mnist_urls[3][1])\n",
    "]\n",
    "torchvision.datasets.MNIST.resources = new_resources\n",
    "\n",
    "# Config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model\n",
    "model = SimpleMLP(num_classes=10)\n",
    "\n",
    "# CL Benchmark Creation\n",
    "perm_mnist = PermutedMNIST(n_experiences=5)\n",
    "train_stream = perm_mnist.train_stream\n",
    "test_stream = perm_mnist.test_stream\n",
    "\n",
    "# Prepare for training & evaluation\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Joint training strategy\n",
    "joint_train = JointTraining(\n",
    "    model, optimizer, criterion, train_mb_size=32, train_epochs=1,\n",
    "    eval_mb_size=32, device=device)\n",
    "\n",
    "# train and eval loop\n",
    "results = []\n",
    "joint_train.train(train_stream)\n",
    "results.append(joint_train.eval(test_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Customization\n",
    "\n",
    "If the standard training and evaluation flows, for some reason, do not fit in your ideal continual learning strategy, you're free to change them as well. You can change the existing flow just by _redefining the train and eval_ methods or any method called in the flow. Everthing is possibile in _Avalanche_! :-\\)\n",
    "\n",
    "In this case we suggest to start by taking a closer look at the inner functioning of the `BaseStrategy` and `JointTraining` classes. We will add more base strategy from which to inherit with different flows as we see they become more needed by the community.\n",
    "\n",
    "Otherwise, you can just ask us to change the current training and evaluation flows opening a new issue, and we will make sure to help you!\n",
    "\n",
    "## 🔌 Plugins\n",
    "\n",
    "As we previously hinted **Plugins** are modules implementing _some specific behaviors you can use to build your strategy more easily_.\n",
    "\n",
    "The basic idea is to **attach them to the main strategy so that they can augment its behaviors.** In practice this means that in the flow execution, for every method of the main strategy, the plugin\\(s\\) corresponding methods **will be called first** \\(if implemented\\).\n",
    "\n",
    "Under the hoods, the `BaseStrategy`, for each method of the flows, calls the respective methods for each of the plugins attached to it.\n",
    "\n",
    "An example of Plugin available in _Avalanche_ is the **Evaluation Plugin**. This plugin will handle automatically all metrics computation and logging. A plugin can be added to the `BaseStrategy` simply by passing it to the constructor thought the name parameter `plugins`.\n",
    "\n",
    "At the moment, in _Avalanche_ are available the following plugins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.training.plugins import ReplayPlugin, \\\n",
    "GDumbPlugin, EvaluationPlugin, CWRStarPlugin, MultiHeadPlugin, LwFPlugin, \\\n",
    "AGEMPlugin, GEMPlugin, EWCPlugin, SynapticIntelligencePlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Plugin\n",
    "\n",
    "Creating a plugin is rather straightforward in _Avalanche_. You simply need to create a class inheriting from `StrategyPlugin` and implement the callbacks of your choice. This is, for example, the implementation of the replay plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.training.plugins import StrategyPlugin\n",
    "\n",
    "class ReplayPlugin(StrategyPlugin):\n",
    "    \"\"\"\n",
    "    Experience replay plugin.\n",
    "\n",
    "    Handles an external memory filled with randomly selected\n",
    "    patterns and implements the \"adapt_train_dataset\" callback to add them to\n",
    "    the training set.\n",
    "\n",
    "    The :mem_size: attribute controls the number of patterns to be stored in\n",
    "    the external memory. We assume the training set contains at least\n",
    "    :mem_size: data points.\n",
    "    \"\"\"\n",
    "    def __init__(self, mem_size=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mem_size = mem_size\n",
    "        self.ext_mem = None\n",
    "        self.it = 0\n",
    "        self.rm_add = None\n",
    "\n",
    "    def adapt_train_dataset(self, strategy, **kwargs):\n",
    "        \"\"\"\n",
    "        Expands the current training set with datapoint from\n",
    "        the external memory before training.\n",
    "        \"\"\"\n",
    "\n",
    "        # Additional set of the current batch to be concatenated to the ext.\n",
    "        # memory at the end of the training\n",
    "        self.rm_add = None\n",
    "\n",
    "        # how many patterns to save for next iter\n",
    "        h = min(self.mem_size // (self.it + 1), len(strategy.current_data))\n",
    "\n",
    "        # We recover it using the random_split method and getting rid of the\n",
    "        # second split.\n",
    "        self.rm_add, _ = random_split(\n",
    "            strategy.current_data, [h, len(strategy.current_data) - h]\n",
    "        )\n",
    "\n",
    "        if self.it > 0:\n",
    "            # We update the train_dataset concatenating the external memory.\n",
    "            # We assume the user will shuffle the data when creating the data\n",
    "            # loader.\n",
    "            strategy.current_data = ConcatDataset([strategy.current_data,\n",
    "                                                   self.ext_mem])\n",
    "\n",
    "    def after_training(self, strategy, **kwargs):\n",
    "        \"\"\" After training we update the external memory with the patterns of\n",
    "         the current training batch/task. \"\"\"\n",
    "\n",
    "        # replace patterns in random memory\n",
    "        ext_mem = self.ext_mem\n",
    "        if self.it == 0:\n",
    "            ext_mem = copy.deepcopy(self.rm_add)\n",
    "        else:\n",
    "            _, saved_part = random_split(\n",
    "                ext_mem, [len(self.rm_add), len(ext_mem)-len(self.rm_add)]\n",
    "            )\n",
    "            ext_mem = ConcatDataset([saved_part, self.rm_add])\n",
    "        self.ext_mem = ext_mem\n",
    "        self.it += 1\n",
    "\n",
    "\n",
    "__all__ = ['ReplayPlugin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, isn't it? :-\\)\n",
    "\n",
    "In general we recommend to _implement a Strategy via plugins_, if possible. This will allow other people to use them and facilitate interoperability among different approaches.\n",
    "\n",
    "For example, we could have implemented a replay strategy as a standalone child class of the `BaseStrategy`. However, creating a plugin to be used in conjunction with the `Naive` strategy is much better, since _experience replay_ is part of many different continual learning strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.training.strategies import Naive\n",
    "\n",
    "cl_strategy = Naive(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=100, train_epochs=4, eval_mb_size=100,\n",
    "    plugins=[ReplayPlugin(mem_size=10000)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the \"_Training_\" chapter for the \"_From Zero to Hero_\" series. We hope you enjoyed it!\n",
    "\n",
    "## 🤝 Run it on Google Colab\n",
    "\n",
    "You can run _this chapter_ and play with it on Google Colaboratory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContinualAI/colab/blob/master/notebooks/avalanche/3.-training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}